"""
æ½®æ±ãƒ‡ãƒ¼ã‚¿å–å¾—å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
GitHub Actionsã§æ¯æœå®Ÿè¡Œã—ã¦æœ€æ–°ã®æ½®æ±äºˆæ¸¬ã‚’å–å¾—
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta

# --- è¨­å®š ---
AREA_CODE = "4419"      # å›½æ±æ¸¯ã®åœ°åŸŸã‚³ãƒ¼ãƒ‰
BACK_PARAM = "3"
DAYS_TO_FETCH = 7       # 7æ—¥åˆ†å–å¾—
FILE_NAME = "tide_prediction.json"

def fetch_tide_data():
    """
    æµ·ä¸Šä¿å®‰åºã‹ã‚‰æ½®æ±ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    """
    all_tide_data = []
    start_date = datetime.now().date()
    BASE_URL = "https://www1.kaiho.mlit.go.jp/TIDE/pred2/cgi-bin/TidePredCgi.cgi"
    
    print(f"ğŸŒŠ æ½®æ±äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ï¼ˆå›½æ±æ¸¯, {DAYS_TO_FETCH}æ—¥åˆ†ï¼‰")
    print(f"å–å¾—æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 70)
    
    for i in range(DAYS_TO_FETCH):
        target_date = start_date + timedelta(days=i)
        
        params = {
            'area': AREA_CODE,
            'back': BACK_PARAM,
            'year': target_date.strftime('%Y'),
            'month': target_date.strftime('%m'),
            'day': target_date.strftime('%d')
        }
        
        current_date_fmt = target_date.strftime('%Y-%m-%d')
        print(f"  å–å¾—ä¸­: {current_date_fmt}", end="")
        
        try:
            response = requests.get(BASE_URL, params=params, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, 'html.parser')
            
            target_table = soup.find('table', bgcolor="#e3ffe3")
            
            if target_table:
                rows = target_table.find_all('tr')
                
                # 0-11æ™‚ã®ãƒ‡ãƒ¼ã‚¿
                hours_0_11 = [td.text.strip() for td in rows[0].find_all('td')[1:]]
                levels_0_11 = [td.text.strip() for td in rows[1].find_all('td')[1:]]
                
                # 12-23æ™‚ã®ãƒ‡ãƒ¼ã‚¿
                hours_12_23 = [td.text.strip() for td in rows[2].find_all('td')[1:]]
                levels_12_23 = [td.text.strip() for td in rows[3].find_all('td')[1:]]
                
                hours = hours_0_11 + hours_12_23
                levels = levels_0_11 + levels_12_23
                
                day_count = 0
                for j in range(24):
                    time_str = f"{hours[j].zfill(2)}:00"
                    level_cm = levels[j].replace(' ', '')
                    
                    all_tide_data.append({
                        "date": current_date_fmt,
                        "time": time_str,
                        "level_cm": int(level_cm)
                    })
                    day_count += 1
                
                print(f" âœ“ ({day_count}ä»¶)")
            else:
                print(f" âœ— ãƒ†ãƒ¼ãƒ–ãƒ«ç‰¹å®šå¤±æ•—")
                
        except Exception as e:
            print(f" âœ— ã‚¨ãƒ©ãƒ¼: {e}")
    
    print("-" * 70)
    print(f"åˆè¨ˆ {len(all_tide_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
    
    return all_tide_data


def save_tide_data(data):
    """
    æ½®æ±ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    output = {
        'updated_at': datetime.now().isoformat(),
        'source': 'æµ·ä¸Šä¿å®‰åº å›½æ±æ¸¯',
        'area_code': AREA_CODE,
        'data': data
    }
    
    with open(FILE_NAME, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜: {FILE_NAME}")


def analyze_tide_summary(data):
    """
    å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ç°¡æ˜“ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    """
    if not data:
        return
    
    print("\nğŸ“Š æ½®æ±ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼:")
    print("-" * 70)
    
    # æ—¥åˆ¥ã®çµ±è¨ˆ
    dates = sorted(set(item['date'] for item in data))
    
    for date in dates[:3]:  # æœ€åˆã®3æ—¥åˆ†ã‚’è¡¨ç¤º
        day_data = [item for item in data if item['date'] == date]
        levels = [item['level_cm'] for item in day_data]
        
        max_level = max(levels)
        min_level = min(levels)
        max_time = day_data[levels.index(max_level)]['time']
        min_time = day_data[levels.index(min_level)]['time']
        
        print(f"\n  ğŸ“… {date}")
        print(f"    æº€æ½®: {max_time} ({max_level}cm)")
        print(f"    å¹²æ½®: {min_time} ({min_level}cm)")
        print(f"    æ½®ä½å·®: {max_level - min_level}cm")
    
    if len(dates) > 3:
        print(f"\n  ... ä»– {len(dates) - 3}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿")


def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("=" * 70)
    print("ğŸŒŠ æ½®æ±ãƒ‡ãƒ¼ã‚¿è‡ªå‹•å–å¾—ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 70)
    print()
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    tide_data = fetch_tide_data()
    
    if tide_data:
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        save_tide_data(tide_data)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        analyze_tide_summary(tide_data)
        
        print("\n" + "=" * 70)
        print("ğŸ‰ æ½®æ±ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ãƒ»ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("=" * 70)
    else:
        print("\nâš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        print("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        exit(1)


if __name__ == "__main__":
    main()
